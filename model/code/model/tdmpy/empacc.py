# Import required packages
from asyncio.log import logger
import openmatrix as omx
import numpy as np
import pandas as pd
from .base import disagg_model

class employment_access(disagg_model):
    def __init__(self,**kwargs):
        super().__init__(**kwargs)    
        if "init" in kwargs:
            if kwargs["init"]:
                self.init_logger()
        else:
            pass

        logger = self.add_logger(name=__name__)
        self.logger = logger
        logger.debug("arguments passed in %s" %kwargs)
        
        # Clear contents of database 'emp_access' table
        self.db._raw_query("delete from emp_access;")
        # initialize the status
        self.status_pct = [0, 0, 10, 31, 44, 89, 95, 99, 100]
        
    def run(self):
        """
         The standard run() method. Overrriding of run() method in the subclass of thread
        """
        
        print("Starting " + self.name)
        self.status_updater(0, "Preparing component" )
        try:   
            if self.args['acc_load']==1:
                self.load_emp_access() 
            else:
                self.calc_employment_access()
            self.run_summaries()
            self.status_updater(100, "Closing component" )
            print("Exiting " + self.name)
            if self.popup == None:
                raise SystemExit()
            elif self.popup.runwithin == "others" :
                raise SystemExit()

        except Exception as e:
            import traceback
            errfile = self.args["OutputFolder"] +'\\_logs\\' + "py.err"
            with open(errfile,"a") as file:
                traceback.print_exc(file=file)

            self.status_updater(-1, "**Error**: Click cancel to check the error message %s"%str(e) )
   

            
    def calc_employment_access(self):
        """
        calc_employment_density: calculate employment density for each TAZ
        inputs:         auto skim OMX file
                        transit-walk skim OMX file
                        block_sed database table (indirectly)
                        taz_allocation database table (indirectly)
        outputs:        emp_access database table
        side effects:   N/A
        calls:          generate_taz_employment_df
        returns:        None
        """
        
        # TODO: Determine if these values are read from a model parameter(s)
        auto_times = [10, 30]
        transit_times = [30]
        self.status_updater(self.status_pct[1], "EmpAcc sub step: calc_emp_access" )
        # Read the auto travel time (AM) matrix into a numpy array
        # Note: model parameter specifies '.mtx' file; we read the corresponding '.omx' file
        auto_skim_fn = self.args["HighwaySkims - am"].replace('.mtx','.omx')
        auto_omx = omx.open_file(auto_skim_fn)
        auto_da_skim_name = 'da_time' # 'drive alone' time
        auto_mtx = auto_omx[auto_da_skim_name]
        auto_arr = np.array(auto_mtx)
        
        # Read the transit-walk time matrices into numpy arrays
        self.status_updater(self.status_pct[2], "EmpAcc sub step:  Read the transit-walk time matrices" )
        transit_walk_skim_fn = self.args["TransitWalkSkims - am"].replace('.mtx','.omx')
        tw_omx = omx.open_file(transit_walk_skim_fn)
        tw_iwait_mtx = tw_omx['iwait']
        tw_iwait_arr = np.array(tw_iwait_mtx)  
        tw_ivtt_mtx = tw_omx['ivtt']
        tw_ivtt_arr = np.array(tw_ivtt_mtx)  
        tw_xwait_mtx = tw_omx['xwait']
        tw_xwait_arr = np.array(tw_xwait_mtx)  
        tw_walk_mtx = tw_omx['walk']
        tw_walk_arr = np.array(tw_walk_mtx) 
        
        # Read the gen_cost array. 
        # We use it to convert various artifacts generated by TransCAD
        # (i.e., NaNs, values close to negative infintiy or positive infinity)
        # to 0's in the transit skim arrays.
        self.status_updater(self.status_pct[3], "EmpAcc sub step:  Read the gen_cost array." )
        tw_gen_cost_mtx = tw_omx['gen_cost']
        tw_gen_cost_arr = np.array(tw_gen_cost_mtx)
        
        num_tazes = tw_omx.shape()[0]
        zeros = np.zeros((num_tazes, num_tazes))
        
        tw_iwait_arr = np.where(tw_gen_cost_arr <= 0.0, zeros, tw_iwait_arr)
        tw_ivtt_arr = np.where(tw_gen_cost_arr <= 0.0, zeros, tw_ivtt_arr)
        tw_xwait_arr = np.where(tw_xwait_arr <= 0.0, zeros, tw_xwait_arr)
        tw_walk_arr = np.where (tw_gen_cost_arr <= 0.0, zeros, tw_walk_arr)
        
        # Sum the 4 elements of transit time to form the total transit travel time
        self.status_updater(self.status_pct[4], "EmpAcc sub step: form the total transit travel time" )
        transit_arr = tw_iwait_arr + tw_ivtt_arr + tw_xwait_arr + tw_walk_arr
        
        
        # Capture TAZ (zone) mappings from the 'auto' OMX in a dict
        skim_index_name = 'Origin'
        taz_map = auto_omx.mapping(skim_index_name)
        taz_keys = list(taz_map.keys())
        taz_vals = list(taz_map.values())
        
        # ASSERT: 'Origin' mapping in auto skim is identical to 'RCIndex' mapping in transit skim.
        
        # DEBUG
        self.logger.debug("Info for auto 'Origin' skim:")
        self.logger.debug('taz_keys min value = ' + str(min(taz_keys)))
        self.logger.debug('taz_values min value = ' + str(min(taz_vals)))        
        self.logger.debug('taz_keys max value = ' + str(max(taz_keys)))
        self.logger.debug('taz_values max value = ' + str(max(taz_vals)))
        

        # Close the OMX's - we're done reading from them
        auto_omx.close()
        tw_omx.close()        
        
        # Create a dataframe from the TAZ mapping keys, with the TAZ mapping values as the index
        taz_map_df = pd.DataFrame(taz_keys, index=taz_vals, columns=['Origin'])
        taz_map_df = taz_map_df.rename(columns={'Origin' : 'taz_id'})
        
        # Calc per-TAZ employment
        emp_df = self.generate_taz_employment_df()
        
        # Merge the taz_map and the per-taz 'emp_df' dataframes on the taz column
        # performing an outer join and retaining each row from the taz map.
        # The resulting dataframe will contain employment counts for each TAZ present in the skim matrices.
        emp_df2 = pd.merge(taz_map_df, emp_df, how="left", left_on="taz_id", right_on='taz_id')
        
        # Employment data will be missing for some TAZes.
        # We must convert any NaNs/NULLs in 'emp_df2' to zeros before doing any further calculations.
        emp_df2 = emp_df2['total_jobs_taz'].fillna(0.0)
              
        # Convert the employment dataframe to a numpy array
        emp_df2 = emp_df2.drop(columns=['taz_id']) 
        emp_arr = emp_df2.to_numpy()

        tot_emp = np.sum(emp_arr)
        self.logger.debug('Total employment = ' + str(tot_emp))

        # Initialize the output dataframe as a copy of the taz mapping keys dataframe
        emp_access_df = taz_map_df

        # Iterate over the auto travel time thresholds: 10 minutes and 30 minutes
        self.status_updater(self.status_pct[5], "EmpAcc sub step: Iterate over the auto travel time threshold" )
        for time in auto_times:
            # create an array of 0/1 flags based on whether o-d travel time is within threshold
            flag_arr = np.where(auto_arr==0,0,(np.where(auto_arr<=time,1,0)))
            # multiply the flags by the destination zone employment
            # we need to transpose the flag array first, then transpose the result
            od_emp_arr = ((flag_arr.T) * emp_arr).T
            # sum the employment within the travel time threshold by origin zone
            o_emp_arr = np.sum(od_emp_arr,0)          
            # calculate the percentage of regional employment within travel time threshold
            o_emp_arr2 = o_emp_arr / tot_emp   
            # add the TAZ #s to the array of employment within time threshold by origin taz
            o_emp_arr3 = np.array([taz_keys,o_emp_arr2])
            # transpose the array
            o_emp_arr4 = o_emp_arr3.T
            # convert the array to a dataframe
            o_emp_df = pd.DataFrame(o_emp_arr4, columns=['taz_id', 'pctemp'+str(time)+'a'])
            # merge the dataframe into the emp_access_df dataframe
            emp_access_df = pd.merge(emp_access_df, o_emp_df, how="left", left_on="taz_id", right_on="taz_id")
        
        # Iterate over the transit travel time thresholds
        # At the moment, we have only one such threshold: 30 minutes
        self.status_updater(self.status_pct[6], "EmpAcc sub step: Iterate over the transit travel time threshold")
        for time in transit_times:
            # create an array of 0/1 flags based on whether o-d travel time is within threshold
            flag_arr = np.where(transit_arr==0,0,(np.where(transit_arr<=time,1,0)))
            # multiply the flags by the destination zone employment
            # we need to transpose the flag array first, then transpose the result
            od_emp_arr = ((flag_arr.T) * emp_arr).T
            # sum the employment within the travel time threshold by origin zone
            o_emp_arr = np.sum(od_emp_arr,0)
            # calculate the percentage of regional employment within travel time threshold
            o_emp_arr2 = o_emp_arr / tot_emp           
            # add the TAZ #s to the array of employment within time threshold by origin taz
            o_emp_arr3 = np.array([taz_keys,o_emp_arr2])
            # transpose the array
            o_emp_arr4 = o_emp_arr3.T
            # convert the array to a dataframe
            o_emp_df = pd.DataFrame(o_emp_arr4, columns=['taz_id', 'pctemp'+str(time)+'t'])
            # merge the dataframe into the emp_access_df dataframe
            emp_access_df = pd.merge(emp_access_df, o_emp_df, how="left", left_on="taz_id", right_on="taz_id")
        
        # Export final dataframe to the database 'emp_access' table
        self.logger.debug('Head of final emp_access_df:')
        self.logger.debug(emp_access_df.head(20))      
        emp_access_df.to_sql(name="emp_access",con=self.db_conn,if_exists="append",index=False)

        csv_fn = self.args['OutputFolder']  + '\\_networks\\' + 'emp_access_taz.csv'
        emp_access_df.to_csv(csv_fn, index=False)
        
        return None
    # end_def calc_employment_access()
    
    def generate_taz_employment_df(self):
        """
        generate_taz_employment_df: generate a DF containing per-TAZ employment;
                                    This is a helper function called by 'calc_employment_access'.
        inputs:         block_sed database table
                        taz_allocation database table
        outputs:        N/A
        side effects:   N/A
        returns:        TAZ employment dataframe
        """
        logger.debug("Entering 'generate_taz_employment_df'.")
         
        # Read the employment data from the 'block_sed' DB table - this data is at the block level,
        # and prep to calculate each TAZ's share 
        query_string = "SELECT * from block_sed;"
        block_sed_df = self.db._raw_query(qry=query_string)
        block_sed_df = block_sed_df[['block_id', 'total_jobs']]
        block_sed_df = block_sed_df.rename(columns={'total_jobs' : 'total_jobs_block'})
        block_sed_df['total_jobs_taz'] = 0
        
        self.logger.debug('Head of block_sed table:')
        self.logger.debug(block_sed_df.head(20))          
        
        # Read the block allocation table from the database
        query_string = "SELECT * from taz_block_allocation;"
        block_alloc_df = self.db._raw_query(qry=query_string)
        
        self.logger.debug('Head of block allocation table:')
        self.logger.debug(block_alloc_df.head(20))
        
        # Join block alloc and block_sed tables,
        # and calculate each TAZ's total employment, based on each of its block's total employment and its share       
        temp_df_1 = pd.merge(left=block_alloc_df, right=block_sed_df, how="left", left_on="block_id", right_on="block_id")
        temp_df_1['tot_jobs_frm_block'] = temp_df_1.apply(lambda row: row['total_jobs_block'] * row['area_fct'], axis=1)        
        
        self.logger.debug('Head of merged table after calc:')
        self.logger.debug(temp_df_1.head(20))
        
        # Aggregate all block's contribution, and form total employment for each TAZ
        temp_df_1 = temp_df_1.set_index(['taz_id'])
        taz_employment_df = temp_df_1.groupby(['taz_id']).agg(total_jobs_taz = ('tot_jobs_frm_block', 'sum'))
                
        self.logger.debug('Head of taz_employment_df:')
        self.logger.debug(taz_employment_df.head(20))
        
        if self.args['loglevel'] == "DEBUG":
            debug_csv_fn = self.args['OutputFolder']  + '\\_networks\\' + 'taz_employment_table.csv'
            taz_employment_df.to_csv(debug_csv_fn, index=True)
        
        return taz_employment_df
    # end_def generate_taz_employment_df
    

    def load_emp_access(self):
        """
        [Populate the emp_access DB table from the emp_access CSV file.]
        """
        path_emp_acc = self.args['emp_acc_load']
        self.emp_acc_df = pd.read_csv(path_emp_acc)
        self.logger.debug("Populating 'emp_access' table.")
        self.emp_acc_df.to_sql(name="emp_access",con=self.db_conn,if_exists="append",index=False) 
    


    def run_summaries(self):
        """
        [run_summaries: generate summary report of employment access calculations]
        inputs:     database "emp_access" table
        outputs:    employment_access_summary.csv file
        returns:    None
        """
        query_string = "SELECT * from emp_access;"
        ea_df = self.db._raw_query(qry=query_string)
        
        ea_auto_10_df = ea_df[ea_df['pctemp10a'] != 0]
        ea_auto_30_df = ea_df[ea_df['pctemp30a'] != 0]
        ea_transit_30_df = ea_df[ea_df['pctemp30t'] != 0]
        self.status_updater(self.status_pct[7], "EmpAcc sub step:  generate summary report ")
        
        data = { 'data'      : [ 'Auto access <= 10 minutes',  'Auto access <= 30 minutes', 'Transit access <= 30 minutes' ],           
                 'num_tazes' : [ len(ea_auto_10_df), len(ea_auto_30_df), len(ea_transit_30_df) ],
                 'average'   : [ ea_auto_10_df['pctemp10a'].mean(), ea_auto_30_df['pctemp30a'].mean(), ea_transit_30_df['pctemp30t'].mean() ],
                 'maximum'   : [ ea_auto_10_df['pctemp10a'].max(),  ea_auto_30_df['pctemp30a'].max(),  ea_transit_30_df['pctemp30t'].max()  ] }
        summary_df = pd.DataFrame(data)
        
        self.logger.debug("Contents of employment access summary DF:\n")
        self.logger.debug(summary_df.head(10))
        
        output_csv_fn = self.args['OutputFolder']  + '\\_summary\\zonal\\' + 'employment_access_summary.csv'
        summary_df.to_csv(output_csv_fn, index=False)
        return None
    # end_def run_summaries()    
# end_class employment_access   

if __name__ == "__main__":
    ea = employment_access()    